# 토큰 계산 방식 선택: tiktoken vs 직접 추정

> RAG 챗봇에서 텍스트의 토큰 수를 계산하는 방식을 선택하기 위한 의사결정

## 1. 배경

RAG 챗봇 개발에서 OpenAI API의 토큰 제한(8,191토큰)을 준수하기 위해 텍스트의 토큰 수를 정확히 계산해야 합니다. 특히 한국어 텍스트의 경우 영어와 토큰화 패턴이 달라 부정확한 추정으로 인한 문제가 발생할 수 있습니다.

**문제점:**
- 토큰 제한 초과로 인한 API 호출 실패
- 부정확한 텍스트 청킹
- 비용 예측의 부정확성
- 한국어 텍스트의 특수성 (높은 토큰 밀도)

## 2. 검토 대상

### A. 직접 추정 방식
```typescript
function estimateTokenCountLegacy(text: string): number {
  const charCount = text.length
  const wordCount = text.split(/\s+/).filter(word => word.length > 0).length
  
  const estimatedByChars = charCount * 0.35 // 한국어 토큰 밀도 고려
  const estimatedByWords = wordCount * 1.5
  
  return Math.ceil(Math.max(estimatedByChars, estimatedByWords))
}
```

### B. tiktoken 라이브러리
```typescript
import { get_encoding } from 'tiktoken'

function estimateTokenCount(text: string): number {
  const encoder = get_encoding('cl100k_base') // text-embedding-3-small과 동일
  const tokens = encoder.encode(text)
  return tokens.length
}
```

## 3. 의사결정 과정

### 3.1 성능 비교 테스트 실시

**테스트 환경:**
- 텍스트 길이별 100회 반복 측정
- 한국어 텍스트 위주로 테스트
- CPU: Apple Silicon, Node.js 환경

**성능 테스트 결과:**

| 텍스트 길이 | tiktoken 평균시간 | 직접추정 평균시간 | 속도 차이 | tiktoken 토큰수 | 직접추정 토큰수 | 정확도 오차 |
|-------------|-------------------|-------------------|-----------|-----------------|-----------------|-------------|
| 짧은글(18자) | 0.638ms | 0.001ms | 450배 느림 | 16 | 7 | 56.3% |
| 중간글(93자) | 0.076ms | 0.001ms | 91배 느림 | 62 | 33 | 46.8% |
| 긴글(1,251자) | 0.315ms | 0.010ms | 31배 느림 | 877 | 438 | 50.1% |
| 매우긴글(39,000자) | 8.136ms | 0.312ms | 26배 느림 | 32,001 | 13,650 | 57.3% |

### 3.2 정확도 분석

**직접 추정 방식의 문제점:**
- 한국어 텍스트에서 **46-57% 오차** 발생
- 실제 토큰 수보다 현저히 적게 추정
- 토큰 제한을 잘못 판단할 위험성

**tiktoken의 장점:**
- OpenAI API와 **100% 동일한 토큰화**
- 다국어 텍스트에서도 정확한 계산
- 청킹 시 정밀한 분할 가능

### 3.3 사용자 경험 관점에서의 분석

**성능 차이의 실제 의미:**
- 가장 느린 경우: **8.136ms** (매우 긴 텍스트)
- 일반적인 경우: **0.076-0.315ms**
- **웹 응답 기준**: < 100ms (즉시 반응), < 1초 (일반 응답)

**결론: 성능 차이는 사용자가 인지할 수 없는 수준**

### 3.4 메모리 사용량 비교

- tiktoken: **+0.43MB** 메모리 사용 증가
- 현대 서버 환경에서 무시할 수 있는 수준

### 3.5 비즈니스 영향 분석

**정확도 부족으로 인한 잠재적 문제:**
1. **API 호출 실패**: 토큰 초과로 인한 에러
2. **부적절한 청킹**: 문맥이 끊어진 텍스트 분할
3. **비용 예측 오류**: 실제 비용과 50% 이상 차이
4. **사용자 경험 저하**: 예상치 못한 오류 발생

**tiktoken 도입 비용:**
1. **라이브러리 의존성 추가**: npm install tiktoken
2. **미미한 성능 오버헤드**: 1-8ms
3. **메모리 사용량 증가**: +0.4MB

## 4. 최종 결정: **tiktoken 라이브러리 사용**

### 선택 근거:
1. **정확도 우선**: 46-57% 오차 해결이 1-8ms 성능 차이보다 중요
2. **사용자 경험**: 성능 차이는 인지 불가능하지만 정확도 문제는 실제 오류로 이어짐
3. **유지보수성**: OpenAI와 동일한 토큰화로 일관성 확보
4. **확장성**: 다양한 언어와 모델에 대응 가능

### 기각된 옵션들의 이유:
- **직접 추정 방식**: 정확도 문제가 사용자 경험에 직접적 악영향
- **혼합 방식**: 복잡성 증가 대비 실질적 이익 부족

## 5. 구현 전략

### 5.1 점진적 도입
1. **1단계**: tiktoken 라이브러리 설치 및 기본 함수 교체
2. **2단계**: 기존 추정 방식을 백업으로 유지 (tiktoken 실패 시)
3. **3단계**: 성능 모니터링 및 최적화

### 5.2 안전장치 구현
```typescript
function estimateTokenCount(text: string): number {
  try {
    const encoder = getEncoder()
    return encoder.encode(text).length
  } catch (error) {
    console.warn('tiktoken 실패, 백업 방식 사용:', error)
    return estimateTokenCountFallback(text) // 기존 방식
  }
}
```

### 5.3 성능 모니터링
- 토큰 계산 시간 추적
- 메모리 사용량 모니터링
- 정확도 검증 로그

## 6. 향후 고려사항

### 6.1 재검토 조건
- tiktoken 라이브러리의 심각한 성능 문제 발견
- 새로운 고성능 토큰화 라이브러리 등장
- 서버 리소스 제약이 심각해질 경우

### 6.2 최적화 방안
- 자주 사용되는 텍스트의 토큰 수 캐싱
- 배치 처리 시 tiktoken 인코더 재사용
- 텍스트 길이별 다른 전략 적용 검토

## 7. 성공 기준

### 7.1 정량적 지표
- **토큰 계산 정확도**: 95% 이상 (기존 50% 대비)
- **API 호출 실패율**: 토큰 관련 오류 90% 감소
- **응답 시간 증가**: 10ms 이하 유지

### 7.2 정성적 지표
- 예상치 못한 토큰 관련 오류 발생률 감소
- 텍스트 청킹 품질 향상
- 비용 예측 정확도 향상

---
**작성일**: 2025-08-07  
**작성자**: Claude Code  
**다음 리뷰**: tiktoken 도입 1개월 후 성능 및 정확도 재평가