/**
 * LangChain ê¸°ë°˜ RAG ì„œë¹„ìŠ¤
 * ìë™ LangSmith ì¶”ì ê³¼ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
 */

import { ChatOpenAI } from '@langchain/openai'
import { PromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence, RunnablePassthrough } from '@langchain/core/runnables'
import { StringOutputParser } from '@langchain/core/output_parsers'
import type { BaseMessage } from '@langchain/core/messages'

import { EmbeddingService } from '../openai/embedding.service'
import { PineconeService } from '../pinecone/pinecone.service'
import type { RAGRequest, RAGResponse, RAGSource, StreamingChatRequest, StreamingEvent } from '../../types'
import { RAG_CONFIG, RAG_MESSAGES, OPENAI_MODELS } from '../../constants'

export class LangChainRAGService {
  private chatModel: ChatOpenAI
  private ragChain: RunnableSequence<any, any>

  constructor(
    private embeddingService: EmbeddingService,
    private pineconeService: PineconeService
  ) {
    // LangSmith í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ë° ì„¤ì •
    this.initializeLangSmithTracking()
    
    // LangChain OpenAI ëª¨ë¸ (ìë™ LangSmith ì¶”ì )
    const apiKey = process.env.OPENAI_API_KEY
    if (!apiKey) {
      throw new Error('OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤')
    }
    
    this.chatModel = new ChatOpenAI({
      modelName: OPENAI_MODELS.CHAT,
      temperature: RAG_CONFIG.DEFAULT_TEMPERATURE,
      apiKey: apiKey
    })

    // RAG ì²´ì¸ êµ¬ì„±
    this.ragChain = this.createRAGChain()
  }

  /**
   * LangChain + LangSmith í†µí•© ì¶”ì  ì´ˆê¸°í™” í™•ì¸
   * 
   * ì£¼ì˜: LangSmith ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ í™˜ê²½ë³€ìˆ˜ëŠ” LANGCHAIN_XX í˜•ì‹ ì‚¬ìš©
   * ì´ëŠ” LangChainì˜ ìë™ LangSmith í†µí•© ê¸°ëŠ¥ì´ í•´ë‹¹ ë³€ìˆ˜ëª…ì„ ìš”êµ¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
   */
  private initializeLangSmithTracking() {
    console.log('ğŸ” LangChain LangSmith ìë™ ì¶”ì  í™•ì¸ ì¤‘...')
    
    // LangChainì´ ìš”êµ¬í•˜ëŠ” í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    const tracingEnabled = process.env.LANGCHAIN_TRACING_V2
    const apiKey = process.env.LANGCHAIN_API_KEY
    const project = process.env.LANGCHAIN_PROJECT
    
    console.log('ğŸ“‹ LangChain LangSmith í†µí•© ì„¤ì •:')
    console.log('- LANGCHAIN_TRACING_V2:', tracingEnabled)
    console.log('- LANGCHAIN_API_KEY:', apiKey ? '***ì„¤ì •ë¨***' : 'ë¯¸ì„¤ì •')
    console.log('- LANGCHAIN_PROJECT:', project)
    
    if (tracingEnabled === 'true' && apiKey && project) {
      console.log('âœ… LangChain LangSmith ìë™ ì¶”ì ì´ í™œì„±í™”ë©ë‹ˆë‹¤')
    } else {
      console.log('âš ï¸  LangChain LangSmith ìë™ ì¶”ì ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤')
      console.log('   ì¶”ì ì„ ì›í•œë‹¤ë©´ LANGCHAIN_XX í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”')
    }
  }

  /**
   * LangChain RAG ì²´ì¸ ìƒì„±
   */
  private createRAGChain() {
    // RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    const ragPrompt = PromptTemplate.fromTemplate(`
ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:
1. ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”  
3. ì§ˆë¬¸ì˜ ëŒ€ìƒê³¼ ë¬¸ì„œì˜ ì£¼ì œê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
4. ì§ˆë¬¸ì˜ ëŒ€ìƒê³¼ ë¬¸ì„œì˜ ì£¼ì œê°€ ë‹¤ë¥´ë‹¤ë©´, "ì œê³µëœ ë¬¸ì„œì—ì„œëŠ” [ì§ˆë¬¸ ëŒ€ìƒ]ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
5. ë‹µë³€ ë§ˆì§€ë§‰ì— ì°¸ê³ í•œ ë¬¸ì„œë¥¼ ê°„ë‹¨íˆ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
6. ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”

ë‹µë³€:`)

    // RAG ì²´ì¸ êµ¬ì„± (ìë™ LangSmith ì¶”ì )
    return RunnableSequence.from([
      {
        context: async (input: { question: string }) => {
          // 1. ì„ë² ë”© ìƒì„±
          const embeddingResult = await this.embeddingService.createEmbedding(input.question)
          
          // 2. ë²¡í„° ê²€ìƒ‰
          const searchResults = await this.pineconeService.query(
            embeddingResult.embedding,
            {
              topK: RAG_CONFIG.DEFAULT_TOP_K,
              scoreThreshold: RAG_CONFIG.DEFAULT_SCORE_THRESHOLD
            }
          )

          // 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
          if (searchResults.length === 0) {
            return RAG_MESSAGES.NO_RESULTS
          }

          return searchResults
            .map((result, index) => `[ë¬¸ì„œ ${index + 1}] ${result.metadata?.title || 'ì œëª© ì—†ìŒ'}\n${result.metadata?.content || ''}`)
            .join('\n\n')
        },
        question: new RunnablePassthrough()
      },
      ragPrompt,
      this.chatModel,
      new StringOutputParser()
    ])
  }

  /**
   * ì¼ë°˜ RAG ì§ˆì˜ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)
   */
  async askQuestion(request: RAGRequest): Promise<RAGResponse> {
    const startTime = Date.now()

    try {
      console.log(`LangChain RAG ì§ˆì˜ ì‹œì‘: "${request.question}"`)

      // LangChain ì²´ì¸ ì‹¤í–‰ (ìë™ LangSmith ì¶”ì )
      const answer = await this.ragChain.invoke({
        question: request.question
      })

      // ì†ŒìŠ¤ ì •ë³´ë¥¼ ìœ„í•œ ë³„ë„ ê²€ìƒ‰ (ë©”íƒ€ë°ì´í„°ìš©)
      const embeddingResult = await this.embeddingService.createEmbedding(request.question)
      const searchResults = await this.pineconeService.query(
        embeddingResult.embedding,
        {
          topK: request.maxResults || RAG_CONFIG.DEFAULT_TOP_K,
          scoreThreshold: request.scoreThreshold || RAG_CONFIG.DEFAULT_SCORE_THRESHOLD
        }
      )

      const sources: RAGSource[] = searchResults.map(result => ({
        id: result.id,
        title: (result.metadata?.title && result.metadata.title.trim()) || 'ì œëª© ì—†ìŒ',
        content: result.metadata?.content || '',
        score: result.score,
        url: result.metadata?.url || ''
      }))

      const response: RAGResponse = {
        answer: answer,
        sources: sources,
        metadata: {
          totalSources: searchResults.length,
          processingTime: Date.now() - startTime,
          model: OPENAI_MODELS.CHAT,
          timestamp: new Date().toISOString()
        }
      }

      console.log(`LangChain RAG ì§ˆì˜ ì™„ë£Œ (${response.metadata.processingTime}ms)`)
      return response

    } catch (error) {
      console.error('LangChain RAG ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨:', error)
      throw new Error(`ì§ˆì˜ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`)
    }
  }

  /**
   * ìŠ¤íŠ¸ë¦¬ë° RAG ì§ˆì˜ (ì„ì‹œ: ì¼ë°˜ ì‘ë‹µìœ¼ë¡œ ë³€í™˜)
   * TODO: ì¶”í›„ LangChain ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ê°œì„ 
   */
  async* askQuestionStream(request: StreamingChatRequest): AsyncGenerator<StreamingEvent, void, unknown> {
    try {
      yield {
        type: 'status',
        content: 'ë‹µë³€ ìƒì„± ì¤‘...',
        data: { timestamp: new Date().toISOString() }
      }

      // ì¼ë°˜ askQuestion ì‚¬ìš© í›„ í† í° ë‹¨ìœ„ë¡œ ì „ì†¡
      const response = await this.askQuestion({ question: request.message })
      
      // ë‹µë³€ì„ í•œ ë²ˆì— ì „ì†¡
      yield {
        type: 'token',
        content: response.answer
      }

      yield {
        type: 'sources',
        data: response.sources
      }

      yield {
        type: 'done',
        data: response.metadata
      }

    } catch (error) {
      console.error('LangChain ìŠ¤íŠ¸ë¦¬ë° RAG ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨:', error)
      yield {
        type: 'error',
        content: `ì§ˆì˜ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`,
        data: { timestamp: new Date().toISOString() }
      }
    }
  }
}