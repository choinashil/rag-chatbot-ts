import { HtmlService } from './html.service'
import type { 
  CrawledDocument, 
  CrawlOptions, 
  CrawlSession, 
  CrawlStatistics,
  PageLink,
  CrawlMetadata 
} from '../../types/html'
import { CRAWL_CONSTANTS, DEFAULT_CRAWL_OPTIONS } from './html.constants'
import * as cheerio from 'cheerio'
import { URL } from 'url'

export class HtmlCrawlerService extends HtmlService {
  private visitedUrls: Set<string> = new Set()
  
  /** ì½˜í…ì¸  ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬ë¥¼ ìœ„í•œ í•´ì‹œ ì €ì¥ì†Œ
   * ì œëª© + ì½˜í…ì¸  ê¸¸ì´ ì¡°í•©ìœ¼ë¡œ ë¹ ë¥¸ ì¤‘ë³µ ê²€ì‚¬ ìˆ˜í–‰ */
  private contentHashes: Set<string> = new Set()
  
  private urlQueue: Array<{url: string, depth: number, parentUrl?: string}> = []
  private crawledDocuments: Map<string, CrawledDocument> = new Map()
  private processingPromises: Map<string, Promise<CrawledDocument | null>> = new Map()
  
  
  /** ì‹œì‘ í˜ì´ì§€ì˜ breadcrumb ê¹Šì´
   * ìƒëŒ€ ê¹Šì´ ê³„ì‚° ê¸°ì¤€ì ìœ¼ë¡œ ì‚¬ìš© */
  private startBreadcrumbDepth: number = 0
  
  /**
   * ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘
   */
  async crawlSite(startUrl: string, options?: Partial<CrawlOptions>): Promise<CrawlSession> {
    const crawlOptions: CrawlOptions = { ...DEFAULT_CRAWL_OPTIONS, ...options }
    const sessionId = this.generateSessionId()
    const startTime = new Date().toISOString()
    
    console.log(`ğŸ•·ï¸ í¬ë¡¤ë§ ì‹œì‘: ${startUrl}`)
    console.log(`ğŸ“Š ì„¤ì •: ìµœëŒ€ ê¹Šì´ ${crawlOptions.maxDepth}, ìµœëŒ€ í˜ì´ì§€ ${crawlOptions.maxPages}`)
    console.log('â”€'.repeat(80))
    
    const session: CrawlSession = {
      id: sessionId,
      startUrl,
      options: crawlOptions,
      startTime,
      status: 'running',
      statistics: this.initializeStatistics()
    }
    
    try {
      // ì´ˆê¸°í™”
      this.reset()
      
      // ì‹œì‘ URLì„ visitedì— ì¶”ê°€í•˜ê³  íì— ì¶”ê°€
      const normalizedStartUrl = this.normalizeUrl(startUrl)
      this.visitedUrls.add(normalizedStartUrl)
      this.urlQueue.push({ url: startUrl, depth: 0 })
      
      // í¬ë¡¤ë§ ì‹¤í–‰
      await this.processCrawlQueue(session)
      
      session.status = 'completed'
      console.log('â”€'.repeat(80))
      console.log(`âœ… í¬ë¡¤ë§ ì™„ë£Œ: ${session.statistics.processedPages}ê°œ í˜ì´ì§€ ì²˜ë¦¬`)
      
    } catch (error) {
      console.error(`âŒ í¬ë¡¤ë§ ì‹¤íŒ¨:`, error)
      session.status = 'error'
      throw error
    }
    
    return session
  }
  
  /**
   * í¬ë¡¤ë§ëœ ë¬¸ì„œë“¤ ë°˜í™˜
   */
  getCrawledDocuments(): CrawledDocument[] {
    return Array.from(this.crawledDocuments.values())
  }
  
  /**
   * íŠ¹ì • URLì˜ í¬ë¡¤ë§ëœ ë¬¸ì„œ ë°˜í™˜
   */
  getCrawledDocument(url: string): CrawledDocument | undefined {
    return this.crawledDocuments.get(this.normalizeUrl(url))
  }
  
  /**
   * í¬ë¡¤ë§ í ì²˜ë¦¬
   */
  private async processCrawlQueue(session: CrawlSession): Promise<void> {
    const concurrentPromises: Promise<void>[] = []
    
    while (this.urlQueue.length > 0 && this.crawledDocuments.size < session.options.maxPages) {
      // ë™ì‹œì„± ì œì–´
      if (concurrentPromises.length >= session.options.concurrency) {
        await Promise.race(concurrentPromises)
        // ëª¨ë“  Promiseë¥¼ ìƒˆë¡œ ì‹œì‘
        await Promise.allSettled(concurrentPromises)
        concurrentPromises.length = 0
      }
      
      const queueItem = this.urlQueue.shift()
      if (!queueItem) break
      
      const { url, depth, parentUrl } = queueItem
      
      // íƒìƒ‰ê¹Šì´ ê¸°ì¤€ ìµœëŒ€ ê¹Šì´ í™•ì¸
      if (depth > session.options.maxDepth) {
        session.statistics.skippedPages++
        continue
      }
      
      // ë„ë©”ì¸ ì œí•œ í™•ì¸
      if (!this.isAllowedDomain(url, session.options.domainRestriction)) {
        session.statistics.skippedPages++
        continue
      }
      
      // í˜ì´ì§€ ì²˜ë¦¬ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹œì‘
      const processPromise = this.processPage(url, depth, parentUrl, session)
        .then(document => {
          if (document) {
            // ìƒˆë¡œìš´ ë§í¬ë¥¼ íì— ì¶”ê°€ (ë¡œê·¸ëŠ” processPageì—ì„œ ì²˜ë¦¬)
            this.addLinksToQueue(document.links, depth + 1, url)
          }
        })
        .catch(error => {
          console.warn(`âš ï¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: ${url}`, error)
          session.statistics.errorPages++
        })
      
      concurrentPromises.push(processPromise)
      
      // í¬ë¡¤ë§ ì§€ì—°
      if (session.options.crawlDelay > 0) {
        await this.delayExecution(session.options.crawlDelay)
      }
    }
    
    // ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸°
    await Promise.allSettled(concurrentPromises)
  }
  
  /**
   * ê°œë³„ í˜ì´ì§€ ì²˜ë¦¬
   */
  private async processPage(
    url: string, 
    depth: number, 
    parentUrl: string | undefined,
    session: CrawlSession
  ): Promise<CrawledDocument | null> {
    const startTime = Date.now()
    
    try {
      // ê¸°ë³¸ HTML ë¬¸ì„œ ì¶”ì¶œ
      const simpleDocument = await this.extractFromUrl(url)
      
      // ì‹œì‘ í˜ì´ì§€ì¸ ê²½ìš° breadcrumb ê¹Šì´ ì €ì¥
      if (depth === 0) {
        this.startBreadcrumbDepth = simpleDocument.breadcrumb.length
        console.log(`  ğŸ“‹ ì‹œì‘ breadcrumb ê¹Šì´: ${this.startBreadcrumbDepth}`)
      }
      
      // í˜„ì¬ í˜ì´ì§€ì˜ breadcrumb ê¹Šì´ë¥¼ ê°€ì ¸ì™€ì„œ ìƒëŒ€ ê¹Šì´ ê³„ì‚°
      const currentBreadcrumbDepth = simpleDocument.breadcrumb.length
      const relativeDepth = currentBreadcrumbDepth - this.startBreadcrumbDepth
      
      console.log(`  ğŸ“„ ì²˜ë¦¬ ì¤‘: ${simpleDocument.title} (ìƒëŒ€ê¹Šì´: ${relativeDepth}, íƒìƒ‰ê¹Šì´: ${depth})`)
      
      // ë¶€ëª¨ í˜ì´ì§€ í¬ë¡¤ë§ ì œí•œ í™•ì¸ (ì‹œì‘ í˜ì´ì§€ ì œì™¸, í˜•ì œ í˜ì´ì§€ëŠ” í—ˆìš©)
      if (relativeDepth < 0 && depth > 0 && !session.options.includeParentPages) {
        console.log(`  âš ï¸ ë¶€ëª¨ í˜ì´ì§€ ìŠ¤í‚µ (ìƒëŒ€ê¹Šì´: ${relativeDepth})`)
        session.statistics.skippedPages++
        return null
      }
      
      
      // íš¨ìœ¨ì ì¸ ì¤‘ë³µ ê²€ì‚¬: ì œëª© + ì½˜í…ì¸  ê¸¸ì´ ê¸°ë°˜
      const quickHash = this.generateQuickHash(simpleDocument.title, simpleDocument.content.length)
      if (this.contentHashes.has(quickHash)) {
        console.log(`  âš ï¸ ì¤‘ë³µ ì½˜í…ì¸  ìŠ¤í‚µ: ${simpleDocument.title}`)
        session.statistics.duplicatePages++
        return null // ì¤‘ë³µ ì½˜í…ì¸ ëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
      }
      this.contentHashes.add(quickHash)
      
      // HTMLì—ì„œ ë§í¬ ì¶”ì¶œ
      const html = await this.fetchPage(url)
      const links = this.extractLinks(html, url)
      
      // í¬ë¡¤ë§ ë©”íƒ€ë°ì´í„° ìƒì„±
      const crawlMetadata: CrawlMetadata = {
        crawlId: this.generateCrawlId(),
        sessionId: session.id,
        discoveryMethod: depth === 0 ? 'initial' : 'link',
        processingTime: Date.now() - startTime,
        errorCount: 0
      }
      
      // í¬ë¡¤ëœ ë¬¸ì„œ ìƒì„±
      const crawledDocument: CrawledDocument = {
        ...simpleDocument,
        id: this.generateDocumentId(url),
        parentUrl,
        depth,
        discoveredAt: new Date().toISOString(),
        links,
        crawlMetadata
      }
      
      this.crawledDocuments.set(this.normalizeUrl(url), crawledDocument)
      session.statistics.processedPages++
      
      
      // í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
      this.updateAverageProcessingTime(session.statistics, crawlMetadata.processingTime)
      
      // í•˜ìœ„ ë§í¬ë¥¼ íì— ì¶”ê°€
      const addedCount = this.addLinksToQueue(crawledDocument.links, depth + 1, url)
      if (addedCount > 0) {
        console.log(`  ğŸ”— ${addedCount}ê°œ í•˜ìœ„ ë§í¬ë¥¼ íì— ì¶”ê°€`)
      }
      
      console.log(`  âœ… ì™„ë£Œ (${crawlMetadata.processingTime}ms)`)
      return crawledDocument
      
    } catch (error) {
      console.error(`  âŒ ì‹¤íŒ¨: ${url}`, error)
      session.statistics.errorPages++
      return null
    }
  }
  
  /**
   * HTMLì—ì„œ ë§í¬ ì¶”ì¶œ
   */
  private extractLinks(html: string, baseUrl: string): PageLink[] {
    const $ = cheerio.load(html)
    const links: PageLink[] = []
    const baseUrlObj = new URL(baseUrl)
    
    $(CRAWL_CONSTANTS.INTERNAL_LINK_SELECTORS).each((_, element) => {
      const href = $(element).attr('href')
      const text = $(element).text().trim()
      
      if (href && this.isValidLink(href)) {
        try {
          const absoluteUrl = new URL(href, baseUrl).toString()
          const isInternal = this.isInternalLink(absoluteUrl, baseUrlObj.hostname)
          
          links.push({
            url: absoluteUrl,
            text: text || href,
            type: isInternal ? 'internal' : 'external',
            discovered: false
          })
        } catch (error) {
          // URL íŒŒì‹± ì˜¤ë¥˜ ë¬´ì‹œ
        }
      }
    })
    
    return links
  }
  
  /**
   * ë§í¬ë“¤ì„ í¬ë¡¤ë§ íì— ì¶”ê°€
   */
  private addLinksToQueue(links: PageLink[], depth: number, parentUrl: string): number {
    let addedCount = 0
    for (const link of links) {
      if (link.type === 'internal' && !link.discovered) {
        const normalizedUrl = this.normalizeUrl(link.url)
        if (!this.visitedUrls.has(normalizedUrl)) {
          this.visitedUrls.add(normalizedUrl) // visitedì— ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
          this.urlQueue.push({
            url: link.url,
            depth,
            parentUrl
          })
          link.discovered = true
          addedCount++
        }
      }
    }
    return addedCount
  }
  
  /**
   * URL ì •ê·œí™”
   */
  private normalizeUrl(url: string): string {
    try {
      const urlObj = new URL(url)
      // ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì™€ í•´ì‹œ ì œê±° (ì„ íƒì )
      urlObj.search = ''
      urlObj.hash = ''
      return urlObj.toString().replace(/\/$/, '') // ë ìŠ¬ë˜ì‹œ ì œê±°
    } catch {
      return url
    }
  }
  
  /**
   * ë„ë©”ì¸ í—ˆìš© ì—¬ë¶€ í™•ì¸
   */
  private isAllowedDomain(url: string, allowedDomains?: string[]): boolean {
    if (!allowedDomains || allowedDomains.length === 0) return true
    
    try {
      const urlObj = new URL(url)
      return allowedDomains.some(domain => urlObj.hostname.includes(domain))
    } catch {
      return false
    }
  }
  
  /**
   * ìœ íš¨í•œ ë§í¬ ì—¬ë¶€ í™•ì¸
   */
  private isValidLink(href: string): boolean {
    if (!href || href.startsWith('#') || href.startsWith('mailto:') || href.startsWith('tel:')) {
      return false
    }
    return true
  }
  
  /**
   * ë‚´ë¶€ ë§í¬ ì—¬ë¶€ í™•ì¸
   */
  private isInternalLink(url: string, baseHostname: string): boolean {
    try {
      const urlObj = new URL(url)
      return urlObj.hostname === baseHostname || urlObj.hostname.endsWith(`.${baseHostname}`)
    } catch {
      return false
    }
  }
  
  
  /**
   * ì§€ì—° ì‹¤í–‰ (delay ë©”ì„œë“œë¥¼ publicìœ¼ë¡œ ì¬êµ¬í˜„)
   */
  private delayExecution(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms))
  }
  
  /**
   * í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
   */
  private updateAverageProcessingTime(statistics: CrawlStatistics, processingTime: number): void {
    const totalTime = statistics.averageProcessingTime * (statistics.processedPages - 1)
    statistics.averageProcessingTime = (totalTime + processingTime) / statistics.processedPages
  }
  
  /**
   * í†µê³„ ì´ˆê¸°í™”
   */
  private initializeStatistics(): CrawlStatistics {
    return {
      totalPages: 0,
      processedPages: 0,
      skippedPages: 0,
      errorPages: 0,
      duplicatePages: 0,
      averageProcessingTime: 0
    }
  }
  
  /**
   * ìƒíƒœ ì´ˆê¸°í™”
   */
  private reset(): void {
    this.visitedUrls.clear()
    this.contentHashes.clear()
    this.urlQueue = []
    this.crawledDocuments.clear()
    this.processingPromises.clear()
  }
  
  /**
   * ID ìƒì„± í—¬í¼ ë©”ì„œë“œë“¤
   */
  private generateSessionId(): string {
    return `session-${Date.now()}-${Math.random().toString(36).slice(2, 11)}`
  }
  
  private generateCrawlId(): string {
    return `crawl-${Date.now()}-${Math.random().toString(36).slice(2, 11)}`
  }
  
  private generateDocumentId(url: string): string {
    return `doc-${Buffer.from(url).toString('base64').replace(/[^a-zA-Z0-9]/g, '').slice(0, 16)}`
  }
  
  /**
   * íš¨ìœ¨ì ì¸ ì¤‘ë³µ ê²€ì‚¬ë¥¼ ìœ„í•œ ë¹ ë¥¸ í•´ì‹œ ìƒì„±
   * ì œëª© + ì½˜í…ì¸  ê¸¸ì´ ì¡°í•©ìœ¼ë¡œ ì„±ëŠ¥ê³¼ ì •í™•ì„±ì˜ ê· í˜•
   */
  private generateQuickHash(title: string, contentLength: number): string {
    return `${title.trim()}_${contentLength}`
  }
}