import { HtmlCrawlerService } from './html-crawler.service'
import type { CrawlOptions, CrawlSession, CrawledDocument } from '../../types/html'

/**
 * ë‹¤ì¤‘ í˜ì´ì§€ HTML í¬ë¡¤ë§ ì„œë¹„ìŠ¤
 * 
 * HtmlCrawlerServiceë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì´íŠ¸ ì „ì²´ë¥¼ í¬ë¡¤ë§í•˜ê³  ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê³ ìˆ˜ì¤€ ì„œë¹„ìŠ¤
 */
export class HtmlMultiPageCrawlerService {
  private crawlerService: HtmlCrawlerService

  constructor() {
    this.crawlerService = new HtmlCrawlerService()
  }

  /**
   * í¬ë¡¤ë§ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜
   */
  async crawlSite(startUrl: string, options?: Partial<CrawlOptions>): Promise<{
    session: CrawlSession,
    documents: CrawledDocument[]
  }> {
    const session = await this.crawlerService.crawlSite(startUrl, options)
    const documents = this.crawlerService.getCrawledDocuments()
    
    return { session, documents }
  }

  /**
   * íŠ¹ì • URLì˜ í¬ë¡¤ë§ëœ ë¬¸ì„œ ë°˜í™˜
   */
  getCrawledDocument(url: string): CrawledDocument | undefined {
    return this.crawlerService.getCrawledDocument(url)
  }

  /**
   * í¬ë¡¤ë§ ì„¸ì…˜ ê²°ê³¼ ì¶œë ¥ (ì½˜ì†”ìš©)
   */
  displayCrawlResults(session: CrawlSession): void {
    console.log('\n' + '='.repeat(80))
    console.log('ğŸ“Š í¬ë¡¤ë§ ì„¸ì…˜ ê²°ê³¼')
    console.log('='.repeat(80))
    
    console.log(`\nğŸ†” ì„¸ì…˜ ì •ë³´:`)
    console.log(`  ì„¸ì…˜ ID: ${session.id}`)
    console.log(`  ì‹œì‘ URL: ${session.startUrl}`)
    console.log(`  ì‹œì‘ ì‹œê°„: ${session.startTime}`)
    console.log(`  ìƒíƒœ: ${session.status}`)
    
    console.log(`\nâš™ï¸ í¬ë¡¤ë§ ì„¤ì •:`)
    console.log(`  ìµœëŒ€ ê¹Šì´: ${session.options.maxDepth}`)
    console.log(`  ìµœëŒ€ í˜ì´ì§€: ${session.options.maxPages}`)
    console.log(`  ë™ì‹œì„±: ${session.options.concurrency}`)
    console.log(`  í¬ë¡¤ë§ ì§€ì—°: ${session.options.crawlDelay}ms`)
    console.log(`  ë„ë©”ì¸ ì œí•œ: [${session.options.domainRestriction?.join(', ') || 'ì—†ìŒ'}]`)
    console.log(`  ë¶€ëª¨ í˜ì´ì§€ í¬í•¨: ${session.options.includeParentPages}`)
    
    console.log(`\nğŸ“ˆ ì²˜ë¦¬ í†µê³„:`)
    console.log(`  ì´ í˜ì´ì§€ ìˆ˜: ${session.statistics.totalPages}`)
    console.log(`  ì²˜ë¦¬ëœ í˜ì´ì§€: ${session.statistics.processedPages}`)
    console.log(`  ê±´ë„ˆë›´ í˜ì´ì§€: ${session.statistics.skippedPages}`)
    console.log(`  ì˜¤ë¥˜ í˜ì´ì§€: ${session.statistics.errorPages}`)
    console.log(`  ì¤‘ë³µ í˜ì´ì§€: ${session.statistics.duplicatePages}`)
    console.log(`  í‰ê·  ì²˜ë¦¬ ì‹œê°„: ${Math.round(session.statistics.averageProcessingTime)}ms`)
  }
  
  /**
   * í¬ë¡¤ë§ëœ ë¬¸ì„œë“¤ ì¶œë ¥ (ì½˜ì†”ìš©)
   */
  displayDocuments(documents: CrawledDocument[]): void {
    console.log('\n' + '='.repeat(80))
    console.log(`ğŸ“š í¬ë¡¤ë§ëœ ë¬¸ì„œ ëª©ë¡ (${documents.length}ê°œ)`)
    console.log('='.repeat(80))
    
    documents.forEach((doc, index) => {
      console.log(`\n[${index + 1}] ${doc.title}`)
      console.log(`  ğŸ“ URL: ${doc.url}`)
      console.log(`  ğŸ“ ê¹Šì´: ${doc.depth}`)
      console.log(`  ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: ${doc.wordCount.toLocaleString()}ì`)
      console.log(`  ğŸ”— ë°œê²¬ëœ ë§í¬: ${doc.links.length}ê°œ`)
      console.log(`  ğŸ“ Breadcrumb: [${doc.breadcrumb.join(' > ')}]`)
      console.log(`  â±ï¸ ì²˜ë¦¬ ì‹œê°„: ${doc.crawlMetadata.processingTime}ms`)
      
      if (doc.parentUrl) {
        console.log(`  ğŸ‘† ë¶€ëª¨ í˜ì´ì§€: ${doc.parentUrl}`)
      }
      
      // ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 200ì)
      const preview = doc.content.length > 200 
        ? doc.content.substring(0, 200) + '...'
        : doc.content
      console.log(`  ğŸ“„ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: "${preview}"`)
    })
    
    // ë§í¬ ê´€ê³„ ìš”ì•½
    this.displayLinkSummary(documents)
  }
  
  /**
   * ë§í¬ ê´€ê³„ ìš”ì•½ ì¶œë ¥ (ì½˜ì†”ìš©)
   */
  private displayLinkSummary(documents: CrawledDocument[]): void {
    console.log('\n' + '='.repeat(80))
    console.log('ğŸ”— ë§í¬ ê´€ê³„ ë¶„ì„')
    console.log('='.repeat(80))
    
    const totalInternalLinks = documents.reduce((sum, doc) => 
      sum + doc.links.filter(link => link.type === 'internal').length, 0)
    const totalExternalLinks = documents.reduce((sum, doc) => 
      sum + doc.links.filter(link => link.type === 'external').length, 0)
    
    console.log(`\nğŸ“Š ë§í¬ í†µê³„:`)
    console.log(`  ë‚´ë¶€ ë§í¬: ${totalInternalLinks}ê°œ`)
    console.log(`  ì™¸ë¶€ ë§í¬: ${totalExternalLinks}ê°œ`)
    console.log(`  ì´ ë§í¬: ${totalInternalLinks + totalExternalLinks}ê°œ`)
    
    // ì‚¬ì´íŠ¸ ê³„ì¸µ êµ¬ì¡° ì¶œë ¥
    console.log(`\nğŸŒ³ ì‚¬ì´íŠ¸ ê³„ì¸µ êµ¬ì¡°:`)
    const hierarchy = this.buildHierarchy(documents)
    this.printHierarchy(hierarchy, 0)
  }
  
  /**
   * ê³„ì¸µ êµ¬ì¡° ìƒì„±
   */
  private buildHierarchy(documents: CrawledDocument[]): any {
    const hierarchy: any = {}
    
    documents.forEach(doc => {
      if (doc.depth === 0) {
        hierarchy[doc.url] = { document: doc, children: {} }
      }
    })
    
    documents.forEach(doc => {
      if (doc.depth > 0 && doc.parentUrl) {
        const parent = this.findInHierarchy(hierarchy, doc.parentUrl)
        if (parent) {
          parent.children[doc.url] = { document: doc, children: {} }
        }
      }
    })
    
    return hierarchy
  }
  
  /**
   * ê³„ì¸µ êµ¬ì¡°ì—ì„œ ë…¸ë“œ ì°¾ê¸°
   */
  private findInHierarchy(hierarchy: any, url: string): any {
    for (const key in hierarchy) {
      if (key === url) return hierarchy[key]
      const found = this.findInHierarchy(hierarchy[key].children, url)
      if (found) return found
    }
    return null
  }
  
  /**
   * ê³„ì¸µ êµ¬ì¡° ì¶œë ¥
   */
  private printHierarchy(hierarchy: any, depth: number): void {
    const indent = '  '.repeat(depth)
    
    for (const url in hierarchy) {
      const node = hierarchy[url]
      console.log(`${indent}ğŸ“„ ${node.document.title} (${node.document.wordCount}ì)`)
      if (Object.keys(node.children).length > 0) {
        this.printHierarchy(node.children, depth + 1)
      }
    }
  }
}