#!/usr/bin/env npx tsx

/**
 * ë‹¤ì¤‘ í˜ì´ì§€ HTML í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸ (3ë‹¨ê³„)
 * 
 * HtmlMultiPageCrawlerServiceë¥¼ ì‚¬ìš©í•œ ë‹¨ìˆœ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
 */

import { HtmlMultiPageCrawlerService } from '../src/services/html/html-multi-page-crawler.service'
import { DEFAULT_TEST_PAGES, DEFAULT_CRAWL_OPTIONS } from '../src/services/html/html.constants'

// ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
async function main() {
  const startUrl = process.argv[2] || DEFAULT_TEST_PAGES.WEBSITE_DESIGN
  
  // ì»¤ìŠ¤í…€ ì˜µì…˜
  const crawlOptions = {
    ...DEFAULT_CRAWL_OPTIONS,
    maxDepth: 10,        // ê¹Šì´ në‹¨ê³„ê¹Œì§€
    maxPages: 200,       // ìµœëŒ€ ní˜ì´ì§€
    crawlDelay: 1000,   // nì´ˆ ì§€ì—° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
    concurrency: 1     // nê°œ ë™ì‹œ í˜ì´ì§€ ì²˜ë¦¬
  }
  
  console.log(`ğŸš€ í¬ë¡¤ë§ ì‹œì‘: ${startUrl}`)
  console.log(`ğŸ“‹ ì„¤ì •: ê¹Šì´ ${crawlOptions.maxDepth}, ìµœëŒ€ ${crawlOptions.maxPages}í˜ì´ì§€`)
  
  try {
    console.log('='.repeat(80))
    console.log('ğŸ•·ï¸ ë‹¤ì¤‘ í˜ì´ì§€ HTML í¬ë¡¤ëŸ¬ ì‹œì‘')
    console.log('='.repeat(80))
    
    const crawlerService = new HtmlMultiPageCrawlerService()
    
    // í¬ë¡¤ë§ ì‹¤í–‰
    const result = await crawlerService.crawlSite(startUrl, crawlOptions)
    
    // ê²°ê³¼ ì¶œë ¥
    crawlerService.displayCrawlResults(result.session)
    crawlerService.displayDocuments(result.documents)
    
  } catch (error) {
    console.error('âŒ í¬ë¡¤ë§ ì‹¤íŒ¨:', error)
    process.exit(1)
  }
}

// ì§ì ‘ ì‹¤í–‰ ì‹œì—ë§Œ main í•¨ìˆ˜ í˜¸ì¶œ
if (require.main === module) {
  main().catch(error => {
    console.error('ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨:', error)
    process.exit(1)
  })
}