#!/usr/bin/env tsx

/**
 * LangSmith ì§ì ‘ ì‚¬ìš© í…ŒìŠ¤íŠ¸
 * LANGSMITH_* í™˜ê²½ ë³€ìˆ˜ë¡œ ì§ì ‘ ì¶”ì 
 */

import dotenv from 'dotenv'
import path from 'path'
import { traceable } from 'langsmith/traceable'
import OpenAI from 'openai'

// í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
dotenv.config({ path: path.resolve(process.cwd(), 'env/.env.dev') })

async function testLangSmithDirect() {
  console.log('ğŸ” LangSmith ì§ì ‘ ì‚¬ìš© í…ŒìŠ¤íŠ¸ ì‹œì‘...')
  
  // í™˜ê²½ ë³€ìˆ˜ í™•ì¸
  console.log('ğŸ“‹ LangSmith ì§ì ‘ ì‚¬ìš© í™˜ê²½ ë³€ìˆ˜:')
  console.log('- LANGSMITH_TRACING:', process.env.LANGSMITH_TRACING)
  console.log('- LANGSMITH_API_KEY:', process.env.LANGSMITH_API_KEY ? '***ì„¤ì •ë¨***' : 'ë¯¸ì„¤ì •')
  console.log('- LANGSMITH_PROJECT_NAME:', process.env.LANGSMITH_PROJECT_NAME)
  console.log('')

  try {
    // OpenAI í´ë¼ì´ì–¸íŠ¸ (LangChain ì—†ì´)
    const openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    })

    // @traceableë¡œ í•¨ìˆ˜ ë˜í•‘
    const directLLMCall = traceable(
      async (question: string) => {
        const response = await openai.chat.completions.create({
          model: 'gpt-3.5-turbo',
          messages: [{ role: 'user', content: question }],
          temperature: 0.1
        })
        return response.choices[0].message.content
      },
      { name: 'direct_openai_call', run_type: 'llm' }
    )

    console.log('ğŸ§ª LangSmith ì§ì ‘ ì¶”ì  ì‹¤í–‰ ì¤‘...')
    const result = await directLLMCall("LangSmith ì§ì ‘ ì‚¬ìš© í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤")
    
    console.log('âœ… LangSmith ì§ì ‘ ì‚¬ìš© ì™„ë£Œ!')
    console.log('ğŸ“ ì‘ë‹µ:', result?.substring(0, 100) + '...')
    console.log('')
    console.log('ğŸ¯ LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ "direct_openai_call" ì¶”ì ì„ í™•ì¸í•˜ì„¸ìš”!')

  } catch (error) {
    console.error('âŒ LangSmith ì§ì ‘ ì‚¬ìš© ì‹¤íŒ¨:', error)
  }
}

testLangSmithDirect()